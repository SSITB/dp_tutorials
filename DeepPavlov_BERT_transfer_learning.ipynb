{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepPavlov_BERT_transfer_learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XqGbCc8C4UAn"
      },
      "source": [
        "# DeepPavlov: Transfer Learning with BERT\n",
        "\n",
        "Today we will cover following tasks:\n",
        "* classification\n",
        "* tagging (Named Enitity Recognition)\n",
        "* question answering (Stanford Question Answering Dataset)\n",
        "\n",
        "and zero-shot transfer from English to 103 other languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H2WFTCgH4UAp"
      },
      "source": [
        "## BERT input representation\n",
        "Text preprocessing for BERT relies on tokenizing text on subtokens (or WordPieces). Then BERT internally represents each subtoken as sum of three vectors:\n",
        "* subtoken embedding\n",
        "* segment embedding\n",
        "* position embedding\n",
        "\n",
        "<img src=\"https://github.com/deepmipt/dp_tutorials/blob/master/img/BERT_input.png?raw=1\" width=\"75%\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2yMoFE9R4UAq"
      },
      "source": [
        "## BERT for text classification\n",
        "When we want to use BERT model for text classification task we can train only one dense layer on top of the output from the last BERT Transformer layer for special `[CLS]` token.\n",
        "\n",
        "<img src=\"https://github.com/deepmipt/dp_tutorials/blob/master/img/BERT_classification.png?raw=1\" width=\"75%\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUKEWk0BDLWA",
        "colab_type": "text"
      },
      "source": [
        "Install DeepPavlov library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwzl_A0MDJ2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install deeppavlov"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5cA1hCADs5g",
        "colab_type": "text"
      },
      "source": [
        "Install requirements for BERT-based classification model trained to detect insults in [Social Commentary](https://www.kaggle.com/c/detecting-insults-in-social-commentary):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkUU9OTHDrtY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! python -m deeppavlov install insults_kaggle_bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPcVqL1yEuab",
        "colab_type": "text"
      },
      "source": [
        "Download and interact with pre-trained model with CLI:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI_aYO38E0PN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! python -m deeppavlov interact -d insults_kaggle_bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0AHB7nAxIHx",
        "colab_type": "text"
      },
      "source": [
        "Interact with text classification model with DeepPavlov Python API:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGYf5yshE5re",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from deeppavlov import build_model, configs\n",
        "\n",
        "model = build_model(configs.classifiers.insults_kaggle_bert, download=False) # download=True if model is not downloaded yet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvAr9jdUzRj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model(['hey, how are you?', 'You are so stupid!'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEWXw2YNzqh2",
        "colab_type": "text"
      },
      "source": [
        "### Dataformat for classification\n",
        "\n",
        "Let's check training data for  insults classification model. We can get data path from model configuration file from section `dataset_reader`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrVwZWadz52L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from pprint import pprint\n",
        "model_config = json.load(open(configs.classifiers.insults_kaggle_bert))\n",
        "\n",
        "pprint(model_config['dataset_reader'])\n",
        "pprint(model_config['metadata']['variables'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrILmNkN35nD",
        "colab_type": "text"
      },
      "source": [
        "there are three .csv files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF8-DrRR2Q-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls ~/.deeppavlov/downloads/insults_data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60OCJIOZ2MYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! head ~/.deeppavlov/downloads/insults_data/train.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMXaNhQz5CMo",
        "colab_type": "text"
      },
      "source": [
        "If you want to train model on your data you need to create configuration file and set up `data_path` to folder with train.csv, valid.csv, test.csv and change `MODEL_PATH` where to save trained model. Details in [documentation](http://docs.deeppavlov.ai/en/master/components/classifiers.html#how-to-train-on-other-datasets).\n",
        "\n",
        "Train model with CLI:\n",
        "```\n",
        "! python -m deeppavlov train config_name\n",
        "```\n",
        "or in Python\n",
        "```\n",
        "from deeppavlov import train_model\n",
        "model = train_model(model_config)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PHC71FmA4UAx"
      },
      "source": [
        "## BERT for tagging (Named Entity Recognition)\n",
        "\n",
        "BERT model can be used for tagging tasks such like Named Entity Recognition and Part of Speech tagging.\n",
        "We train only one dense layer on top of the output from the last BERT Transformer layer for each token. You can optionally add CRF layer on top the dense layer like in most common architecture BiLSTM + CRF for tagging.\n",
        "\n",
        "Named Entity Recognition:\n",
        "\n",
        "For example, we want to extract persons' and organizations' names from the text. Then for the input text:\n",
        "\n",
        "    Yan Goodfellow works for Google Brain\n",
        "\n",
        "a NER model needs to provide the following sequence of tags:\n",
        "\n",
        "    B-PER I-PER    O     O   B-ORG  I-ORG\n",
        "\n",
        "Where *B-* and *I-* prefixes stand for the beginning and inside of the entity, while *O* stands for out of tag or no tag. Markup with the prefix scheme is called *BIO markup*. This markup is introduced for distinguishing of consequent entities with similar types.\n",
        "\n",
        "Here is how input is preprocessed for tagging:\n",
        "\n",
        "<img src=\"https://github.com/deepmipt/dp_tutorials/blob/master/img/BERT_NER.png?raw=1\" width=\"50%\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaMfNCkT3GDb",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb1IICCG2nRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! python -m deeppavlov interact ner_ontonotes_bert -d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OCQoQ9w2rK8",
        "colab_type": "text"
      },
      "source": [
        "Data for Named Enitity Recognition task is usually stored in CoNLL files.\n",
        "Typical CoNLL file with NER data contains lines with pairs of tokens (word/punctuation symbol) and tags, separated by a whitespace. In many cases additional information such as POS tags included between  Different documents are separated by lines **started** with **-DOCSTART-** token. Different sentences are separated by an empty line. Example\n",
        "\n",
        "    -DOCSTART- -X- -X- O\n",
        "\n",
        "    EU NNP B-NP B-ORG\n",
        "    rejects VBZ B-VP O\n",
        "    German JJ B-NP B-MISC\n",
        "    call NN I-NP O\n",
        "    to TO B-VP O\n",
        "    boycott VB I-VP O\n",
        "    British JJ B-NP B-MISC\n",
        "    lamb NN I-NP O\n",
        "    . . O O\n",
        "\n",
        "    Peter NNP B-NP B-PER\n",
        "    Blackburn NNP I-NP I-PER\n",
        "    \n",
        "    \n",
        "If you want to train model on your own data you can convert it to this CoNLL format or implement your version of `dataset_reader`. As for classification task model can be trained with CLI:\n",
        "```\n",
        "! python -m deeppavlov train config_name\n",
        "```\n",
        "or in Python\n",
        "```\n",
        "from deeppavlov import train_model\n",
        "model = train_model(model_config)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZkTqzVTA4UAy",
        "colab": {}
      },
      "source": [
        "# interact pre-trained model in deeppavlov \n",
        "# dataformat\n",
        "# what to change to train on your own data\n",
        "# commands to run"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rxw8N0TR4UA1"
      },
      "source": [
        "## BERT for Question Answering (Stanford Question Answering Dataset)\n",
        "\n",
        "One can use BERT model for extractive Question Answering, e.g.,\n",
        "context:\n",
        "```markdown\n",
        "In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls under gravity. The main forms of precipitation include drizzle, rain, sleet, snow, graupel and hail… Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals **within a cloud**. Short, intense periods of rain in scattered locations are called “showers”.\n",
        "```\n",
        "and question:\n",
        "```\n",
        "Where do water droplets collide with ice crystals to form precipitation?\n",
        "```\n",
        "Answer is always a span from context.\n",
        "\n",
        "To solve this task with BERT model all we need is to train two dense layes to predict answer start and answer end positions:\n",
        "\n",
        "<img src=\"https://github.com/deepmipt/dp_tutorials/blob/master/img/BERT_QA.png?raw=1\" width=\"50%\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptsXVulc96I9",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b4H6QeKM4UA2"
      },
      "source": [
        "## Zero-shot Transfer from English to 103 languages\n",
        "\n",
        "<img src=\"https://github.com/deepmipt/dp_tutorials/blob/master/img/BERT_multilingual.png?raw=1\" width=\"75%\" />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CgLelDoC4UA_",
        "colab": {}
      },
      "source": [
        "# interact with multi-NER\n",
        "# interact with multi-QA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B3Is4zK17hT3",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iS2_pO7B4UBB",
        "colab": {}
      },
      "source": [
        "! wget https://raw.githubusercontent.com/deepmipt/DeepPavlov/squad_multilingual_configs/deeppavlov/configs/squad/squad_bert_multilingual_freezed_emb.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zb8eVCAL8Jqo",
        "colab": {}
      },
      "source": [
        "! python -m deeppavlov install ./squad_bert_multilingual_freezed_emb.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SBQ1GW9o4UBF",
        "colab": {}
      },
      "source": [
        "! python -m deeppavlov interact -d ./squad_bert_multilingual_freezed_emb.json"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}