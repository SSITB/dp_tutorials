{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial_Day_2_seq2seq.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY2xslekqyb5",
        "colab_type": "text"
      },
      "source": [
        "## You can open the tutorial in a [COLAB](https://colab.research.google.com/github/deepmipt/dp_tutorials/blob/master/russian_tutorials/Tutorial_2_Seq2seq_bot.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYGmSm7PBGeF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8902b3fe-53ec-400b-ef93-9e119cc03261"
      },
      "source": [
        "# for those who work in colab only\n",
        "!pip install deeppavlov"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deeppavlov\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/4f/1f73825653f388ead9a9ea2f46cad51c92bd84a899ebf983906013e14d1c/deeppavlov-0.4.0-py3-none-any.whl (682kB)\n",
            "\r\u001b[K     |▌                               | 10kB 15.2MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |█▍                              | 30kB 3.2MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |██▉                             | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |███▍                            | 71kB 3.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 81kB 4.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 92kB 4.5MB/s eta 0:00:01\r\u001b[K     |████▉                           | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 194kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 204kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 215kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 225kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 235kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 245kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 256kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 266kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 276kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 286kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 296kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 307kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 317kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 327kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 337kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 348kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 358kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 368kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 378kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 389kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 399kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 409kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 419kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 430kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 440kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 450kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 460kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 471kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 481kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 491kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 501kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 512kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 522kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 532kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 542kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 552kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 563kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 573kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 583kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 593kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 604kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 614kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 624kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 634kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 645kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 655kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 665kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 675kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 686kB 3.4MB/s \n",
            "\u001b[?25hCollecting overrides==1.9 (from deeppavlov)\n",
            "  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\n",
            "Collecting fuzzywuzzy==0.16.0 (from deeppavlov)\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/36/be990a35c7e8ed9dc176c43b5699cd971cec0b6f9ef858843374171df4f2/fuzzywuzzy-0.16.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: h5py==2.8.0 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (2.8.0)\n",
            "Collecting numpy==1.14.5 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/1e/116ad560de97694e2d0c1843a7a0075cc9f49e922454d32f49a80eb6f1f2/numpy-1.14.5-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n",
            "\u001b[K     |████████████████████████████████| 12.2MB 44.0MB/s \n",
            "\u001b[?25hCollecting pandas==0.23.1 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/eb/6ab533ea8e35e7dd159af6922ac1123d4565d89f3926ad9a6aa46530978f/pandas-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (11.8MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8MB 32.6MB/s \n",
            "\u001b[?25hCollecting pymorphy2==0.8 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 28.3MB/s \n",
            "\u001b[?25hCollecting requests==2.19.1 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl (91kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.1MB/s \n",
            "\u001b[?25hCollecting tqdm==4.23.4 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/24/6ab1df969db228aed36a648a8959d1027099ce45fad67532b9673d533318/tqdm-4.23.4-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 24.7MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.19.1 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/2d/9fbc7baa5f44bc9e88ffb7ed32721b879bfa416573e85031e16f52569bc9/scikit_learn-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (12.4MB)\n",
            "\u001b[K     |████████████████████████████████| 12.4MB 44.1MB/s \n",
            "\u001b[?25hCollecting flask-cors==3.0.6 (from deeppavlov)\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/db/f3495569d5c3e2bdb9fb8a66c54503364abb6f35a9da2227cf5c9c50dc42/Flask_Cors-3.0.6-py2.py3-none-any.whl\n",
            "Collecting Cython==0.28.5 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/8e/32b280abb0947a96cdbb8329fb2014851a21fc1d099009f946ea8a8202c3/Cython-0.28.5-cp36-cp36m-manylinux1_x86_64.whl (3.4MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 46.5MB/s \n",
            "\u001b[?25hCollecting flask==1.0.2 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/e7/08578774ed4536d3242b14dacb4696386634607af824ea997202cd0edb4b/Flask-1.0.2-py2.py3-none-any.whl (91kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 35.3MB/s \n",
            "\u001b[?25hCollecting rusenttokenize==0.0.4 (from deeppavlov)\n",
            "  Downloading https://files.pythonhosted.org/packages/f8/4c/e2aeee9cdcc266303289ad8c4acfdcf401781646bcc311ee2bf18f84d663/rusenttokenize-0.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: nltk==3.2.5 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (3.2.5)\n",
            "Collecting scipy==1.1.0 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2MB 1.3MB/s \n",
            "\u001b[?25hCollecting flasgger==0.9.1 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/4c/8735be27bebb88b0acbdc9db1d522583db10821aec3d3fb6112df0f41701/flasgger-0.9.1-py2.py3-none-any.whl (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 50.2MB/s \n",
            "\u001b[?25hCollecting pytelegrambotapi==3.5.2 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/5a/7aab147b253f19e5ef007316f39cf693a63d5cd7f654c3805c76f6bde979/pyTelegramBotAPI-3.5.2.tar.gz (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 19.5MB/s \n",
            "\u001b[?25hCollecting pyopenssl==18.0.0 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/af/9d29e6bd40823061aea2e0574ccb2fcf72bfd6130ce53d32773ec375458c/pyOpenSSL-18.0.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 28.9MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/9b/358faaff410f65a4ad159275e897b5956dcb20576c5b8e764b971c1634d7/pymorphy2_dicts_ru-2.4.404381.4453942-py2.py3-none-any.whl (8.0MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0MB 697kB/s \n",
            "\u001b[?25hCollecting keras==2.2.0 (from deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/12/4cabc5c01451eb3b413d19ea151f36e33026fc0efb932bf51bcaf54acbf5/Keras-2.2.0-py2.py3-none-any.whl (300kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 51.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py==2.8.0->deeppavlov) (1.12.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.1->deeppavlov) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.1->deeppavlov) (2.5.3)\n",
            "Collecting dawg-python>=0.7 (from pymorphy2==0.8->deeppavlov)\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4 (from pymorphy2==0.8->deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 49.6MB/s \n",
            "\u001b[?25hCollecting urllib3<1.24,>=1.21.1 (from requests==2.19.1->deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/c9/6fdd990019071a4a32a5e7cb78a1d92c53851ef4f56f62a3486e6a7d8ffb/urllib3-1.23-py2.py3-none-any.whl (133kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 55.5MB/s \n",
            "\u001b[?25hCollecting idna<2.8,>=2.5 (from requests==2.19.1->deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/2a/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165/idna-2.7-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 31.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.19.1->deeppavlov) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.19.1->deeppavlov) (3.0.4)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->deeppavlov) (7.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->deeppavlov) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->deeppavlov) (0.15.4)\n",
            "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->deeppavlov) (2.10.1)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from flasgger==0.9.1->deeppavlov) (3.13)\n",
            "Requirement already satisfied: jsonschema>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from flasgger==0.9.1->deeppavlov) (2.6.0)\n",
            "Requirement already satisfied: mistune in /usr/local/lib/python3.6/dist-packages (from flasgger==0.9.1->deeppavlov) (0.8.4)\n",
            "Collecting cryptography>=2.2.1 (from pyopenssl==18.0.0->deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/18/c6557f63a6abde34707196fb2cad1c6dc0dbff25a200d5044922496668a4/cryptography-2.7-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 49.9MB/s \n",
            "\u001b[?25hCollecting keras-preprocessing==1.0.1 (from keras==2.2.0->deeppavlov)\n",
            "  Downloading https://files.pythonhosted.org/packages/f8/33/275506afe1d96b221f66f95adba94d1b73f6b6087cfb6132a5655b6fe338/Keras_Preprocessing-1.0.1-py2.py3-none-any.whl\n",
            "Collecting keras-applications==1.0.2 (from keras==2.2.0->deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/60/c557075e586e968d7a9c314aa38c236b37cb3ee6b37e8d57152b1a5e0b47/Keras_Applications-1.0.2-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 29.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10->flask==1.0.2->deeppavlov) (1.1.1)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.2.1->pyopenssl==18.0.0->deeppavlov) (1.12.3)\n",
            "Collecting asn1crypto>=0.21.0 (from cryptography>=2.2.1->pyopenssl==18.0.0->deeppavlov)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.2.1->pyopenssl==18.0.0->deeppavlov) (2.19)\n",
            "Building wheels for collected packages: overrides, pytelegrambotapi\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\n",
            "  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/a0/fa/c3539fd47aa9f834230d64039c4bc620463bc7afc39b0f3f35\n",
            "Successfully built overrides pytelegrambotapi\n",
            "\u001b[31mERROR: yellowbrick 0.9.1 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.14.0 has requirement keras-applications>=1.0.6, but you'll have keras-applications 1.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.14.0 has requirement keras-preprocessing>=1.0.5, but you'll have keras-preprocessing 1.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.1.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.5.1 has requirement pandas>=0.23.4, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.5.4 has requirement pandas>=0.23.4, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imbalanced-learn 0.4.3 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=0.24.0, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.21.0, but you'll have requests 2.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.5 has requirement pandas>=0.23.4, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.54 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: overrides, fuzzywuzzy, numpy, pandas, dawg-python, pymorphy2-dicts, pymorphy2, urllib3, idna, requests, tqdm, scikit-learn, flask, flask-cors, Cython, rusenttokenize, scipy, flasgger, pytelegrambotapi, asn1crypto, cryptography, pyopenssl, pymorphy2-dicts-ru, keras-preprocessing, keras-applications, keras, deeppavlov\n",
            "  Found existing installation: numpy 1.16.4\n",
            "    Uninstalling numpy-1.16.4:\n",
            "      Successfully uninstalled numpy-1.16.4\n",
            "  Found existing installation: pandas 0.24.2\n",
            "    Uninstalling pandas-0.24.2:\n",
            "      Successfully uninstalled pandas-0.24.2\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: idna 2.8\n",
            "    Uninstalling idna-2.8:\n",
            "      Successfully uninstalled idna-2.8\n",
            "  Found existing installation: requests 2.21.0\n",
            "    Uninstalling requests-2.21.0:\n",
            "      Successfully uninstalled requests-2.21.0\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "  Found existing installation: scikit-learn 0.21.2\n",
            "    Uninstalling scikit-learn-0.21.2:\n",
            "      Successfully uninstalled scikit-learn-0.21.2\n",
            "  Found existing installation: Flask 1.1.1\n",
            "    Uninstalling Flask-1.1.1:\n",
            "      Successfully uninstalled Flask-1.1.1\n",
            "  Found existing installation: Cython 0.29.12\n",
            "    Uninstalling Cython-0.29.12:\n",
            "      Successfully uninstalled Cython-0.29.12\n",
            "  Found existing installation: scipy 1.3.0\n",
            "    Uninstalling scipy-1.3.0:\n",
            "      Successfully uninstalled scipy-1.3.0\n",
            "  Found existing installation: Keras-Preprocessing 1.1.0\n",
            "    Uninstalling Keras-Preprocessing-1.1.0:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.0\n",
            "  Found existing installation: Keras-Applications 1.0.8\n",
            "    Uninstalling Keras-Applications-1.0.8:\n",
            "      Successfully uninstalled Keras-Applications-1.0.8\n",
            "  Found existing installation: Keras 2.2.4\n",
            "    Uninstalling Keras-2.2.4:\n",
            "      Successfully uninstalled Keras-2.2.4\n",
            "Successfully installed Cython-0.28.5 asn1crypto-0.24.0 cryptography-2.7 dawg-python-0.7.2 deeppavlov-0.4.0 flasgger-0.9.1 flask-1.0.2 flask-cors-3.0.6 fuzzywuzzy-0.16.0 idna-2.7 keras-2.2.0 keras-applications-1.0.2 keras-preprocessing-1.0.1 numpy-1.14.5 overrides-1.9 pandas-0.23.1 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pymorphy2-dicts-ru-2.4.404381.4453942 pyopenssl-18.0.0 pytelegrambotapi-3.5.2 requests-2.19.1 rusenttokenize-0.0.4 scikit-learn-0.19.1 scipy-1.1.0 tqdm-4.23.4 urllib3-1.23\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "idna",
                  "numpy",
                  "pandas",
                  "requests",
                  "tqdm",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0181Jm8A_Ck",
        "colab_type": "text"
      },
      "source": [
        "### Туториал 2: Sequence-to-sequence Бот\n",
        "\n",
        "In this tutorial we are going to implement [sequence-to-sequence](https://arxiv.org/abs/1409.3215) model in DeepPavlov.\n",
        "\n",
        "Sequence-to-sequence is the concept of mapping input sequence to target sequence. Sequence-to-sequence models consist of two main components: encoder and decoder. Encoder is used to encode the input sequence to dense representation and decoder uses this dense representation to generate target sequence.\n",
        "\n",
        "![]()<img src=\"https://cdn-images-1.medium.com/max/1400/1*Ismhi-muID5ooWf3ZIQFFg.png\" width=\"700\"/>\n",
        "\n",
        "(image credit: [towardsdatascience.com](https://towardsdatascience.com))\n",
        "\n",
        "To implement this model in DeepPavlov we have to code some DeepPavlov abstractions:\n",
        "* **DatasetReader** to read the data\n",
        "* **DatasetIterator** to generate batches\n",
        "* **Vocabulary** to convert words to indexes\n",
        "* **Model** to train it and then use it\n",
        "* and some other components for pre- and postprocessing\n",
        "\n",
        "### Probably the most usefull blog post about tensorflow I've seen\n",
        "or why `tf.shape != tensor.get_shape`\n",
        "\n",
        "[TensorFlow: Shapes and dynamic dimensions](https://blog.metaflow.fr/shapes-and-dynamic-dimensions-in-tensorflow-7b1fe79be363)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R68bwCZJA_Cn",
        "colab_type": "text"
      },
      "source": [
        "### Designations\n",
        "    \n",
        "for clarity we add the following suffixes to the end of python variables:\n",
        "\n",
        "- `_ph` - tf.placeholder\n",
        "\n",
        "- `_layer` - tf.keras.layer\n",
        "\n",
        "- `_op` - tensorflow operation (remember that tf.Tensor is not a set of valuet, it is a node in computational graph)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0FDb-unA_Co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pdb\n",
        "import json\n",
        "from itertools import chain\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import deeppavlov\n",
        "from deeppavlov import build_model\n",
        "from deeppavlov.core.data.dataset_reader import DatasetReader\n",
        "from deeppavlov.core.common.registry import register"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0cwXLisA_Cr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c08190d7-7a29-44f0-e745-e402d1443b9d"
      },
      "source": [
        "tf.__version__, deeppavlov.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1.14.0', '0.4.0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQJQPI_6A_Cx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "MAXLEN = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W2g18OQA_C0",
        "colab_type": "text"
      },
      "source": [
        "## Download & extract dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HsWFivcA_C1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "88627a68-3145-4503-9732-a2a568439d6a"
      },
      "source": [
        "from deeppavlov.core.data.utils import download_decompress\n",
        "\n",
        "dataset_path = './personachat'\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    download_decompress('http://files.deeppavlov.ai/datasets/personachat_v2.tar.gz', dataset_path)\n",
        "else:\n",
        "    print('Dataset has already been downloaded')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-07-14 03:02:54.356 INFO in 'deeppavlov.core.data.utils'['utils'] at line 63: Downloading from http://files.deeppavlov.ai/datasets/personachat_v2.tar.gz to personachat/personachat_v2.tar.gz\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "I0714 03:02:54.356867 140328521566080 utils.py:63] Downloading from http://files.deeppavlov.ai/datasets/personachat_v2.tar.gz to personachat/personachat_v2.tar.gz\n",
            "100%|██████████| 223M/223M [00:48<00:00, 4.59MB/s]\n",
            "2019-07-14 03:03:43.16 INFO in 'deeppavlov.core.data.utils'['utils'] at line 201: Extracting personachat/personachat_v2.tar.gz archive into personachat\n",
            "I0714 03:03:43.016792 140328521566080 utils.py:201] Extracting personachat/personachat_v2.tar.gz archive into personachat\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t90K4sfnA_C5",
        "colab_type": "text"
      },
      "source": [
        "## DatasetReader\n",
        "\n",
        "DatasetReader is used to read and parse data from files. Here, we define new PersonaChatDatasetReader which reads [PersonaChat dataset](https://arxiv.org/abs/1801.07243).\n",
        "\n",
        "PersonaChat dataset consists of dialogs and user personalities.\n",
        "\n",
        "User personality is described by four sentences, e.g.:\n",
        "\n",
        "    i like to remodel homes.\n",
        "    i like to go hunting.\n",
        "    i like to shoot a bow.\n",
        "    my favorite holiday is halloween.\n",
        "    \n",
        "But we will be using only dialogues in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ6o96WsA_C6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@register('personachat_dataset_reader')  # to use component later in train config it sould be registered\n",
        "class PersonaChatDatasetReader(DatasetReader):\n",
        "    \"\"\"\n",
        "    PersonaChat dataset from\n",
        "    Zhang S. et al. Personalizing Dialogue Agents: I have a dog, do you have pets too?\n",
        "    https://arxiv.org/abs/1801.07243\n",
        "    Also, this dataset is used in ConvAI2 http://convai.io/\n",
        "    This class reads dataset to the following format:\n",
        "    [{\n",
        "        'persona': [list of persona sentences],\n",
        "        'x': input utterance,\n",
        "        'y': output utterance,\n",
        "        'dialog_history': list of previous utterances\n",
        "        'candidates': [list of candidate utterances]\n",
        "        'y_idx': index of y utt in candidates list\n",
        "      },\n",
        "       ...\n",
        "    ]\n",
        "    \"\"\"\n",
        "    def read(self, dir_path: str, mode='self_original', verbose=False):\n",
        "        if verbose:\n",
        "            print('Reading dataset...')\n",
        "        dir_path = Path(dir_path)\n",
        "        dataset = {}\n",
        "        for dt in ['train', 'valid', 'test']:\n",
        "            dataset[dt] = self._parse_data(dir_path / f'{dt}_{mode}.txt', verbose)\n",
        "\n",
        "        print('Done\\n')\n",
        "        return dataset\n",
        "\n",
        "    @staticmethod\n",
        "    def _parse_data(filename, verbose):\n",
        "        examples = []\n",
        "        if verbose:\n",
        "            print(filename)\n",
        "        curr_persona = []\n",
        "        curr_dialog_history = []\n",
        "        persona_done = False\n",
        "        with filename.open('r') as fin:\n",
        "            for line in fin:\n",
        "                line = ' '.join(line.strip().split(' ')[1:])\n",
        "                your_persona_pref = 'your persona: '\n",
        "                if line[:len(your_persona_pref)] == your_persona_pref and persona_done:\n",
        "                    curr_persona = [line[len(your_persona_pref):]]\n",
        "                    curr_dialog_history = []\n",
        "                    persona_done = False\n",
        "                elif line[:len(your_persona_pref)] == your_persona_pref:\n",
        "                    curr_persona.append(line[len(your_persona_pref):])\n",
        "                else:\n",
        "                    persona_done = True\n",
        "                    x, y, _, candidates = line.split('\\t')\n",
        "                    candidates = candidates.split('|')\n",
        "                    example = {\n",
        "                        'persona': curr_persona,\n",
        "                        'x': x,\n",
        "                        'y': y,\n",
        "                        'dialog_history': curr_dialog_history[:],\n",
        "                        'candidates': candidates,\n",
        "                        'y_idx': candidates.index(y)\n",
        "                    }\n",
        "                    curr_dialog_history.extend([x, y])\n",
        "                    examples.append(example)\n",
        "\n",
        "        return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT5ysKSuA_C9",
        "colab_type": "text"
      },
      "source": [
        "Read dataset, check size and sample some examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWpzy6UvA_C-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d5afdc70-32b6-4499-8fb8-8ac3e9158849"
      },
      "source": [
        "data = PersonaChatDatasetReader().read('./personachat', verbose=True)\n",
        "\n",
        "for k in data:\n",
        "    print(k, '\\t:', len(data[k]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading dataset...\n",
            "personachat/train_self_original.txt\n",
            "personachat/valid_self_original.txt\n",
            "personachat/test_self_original.txt\n",
            "Done\n",
            "\n",
            "train \t: 65719\n",
            "valid \t: 7801\n",
            "test \t: 7512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfZHEvsQA_DB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "5a48c6f7-878c-4228-c016-d77c2a24c00e"
      },
      "source": [
        "data['train'][100]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'candidates': ['my girlfriend eloped w my best friend',\n",
              "  'we should get a cleaner to clean out all the tofu my wife keeps cooking . gross !',\n",
              "  'you sound young and fabulous . thanks for chatting !',\n",
              "  'how do you like new york ?',\n",
              "  'my sweet puppy will have the rings',\n",
              "  'green ones ! ha ha . only because that is my favorite .',\n",
              "  'i like to play soccer , i love games of throne and a vegan',\n",
              "  'ah that is nice . like my turtles . you have pets ?',\n",
              "  'peanut butter on my pizza',\n",
              "  'which show were you in ?',\n",
              "  'i still stay with my parents',\n",
              "  'jokes sometimes goes serious and it is not good',\n",
              "  'i play for midlands high school . our mascot is the cougars',\n",
              "  'that sounds terrible ! but would probably make you rich .',\n",
              "  'oh i like to all the time but everyone wants me everywhere',\n",
              "  'i am super afraid of heights',\n",
              "  'yeah , me and my grandma go there all the time .',\n",
              "  'i like to attend social gatherings .',\n",
              "  'i do not blame you . although i have to watch out for shellfish , i am allergic .',\n",
              "  'that is cute ! how old are they ?'],\n",
              " 'dialog_history': ['hey , what are you up to ?',\n",
              "  'hello , i am listening to lady gaga , do you like her ?',\n",
              "  'i prefer rock music , like led zeppelin .',\n",
              "  'madonna is my first favorite . do you go to a lot of concerts ?',\n",
              "  'i would if i could , but i have a farm to maintain .',\n",
              "  'i work at the mall , so i am close to the venue .',\n",
              "  'i prefer hiking outdoors and photography rather than crowded malls .',\n",
              "  'pays well , lol . i make great money as the manager .',\n",
              "  'work is tiring . i would love to travel the world instead .',\n",
              "  'i would love to travel as well .',\n",
              "  'staying here is fine too though . my two dogs keep me company .',\n",
              "  'i love dogs ! what kind do you have ?'],\n",
              " 'persona': ['i go to at least 10 concerts a year.',\n",
              "  'i work in retail.',\n",
              "  'madonna is my all time favorite.',\n",
              "  'lady gaga is my current favorite singer.'],\n",
              " 'x': 'they are both greyhounds . their names are tom and jerry .',\n",
              " 'y': 'that is cute ! how old are they ?',\n",
              " 'y_idx': 19}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTe6SpNeA_DF",
        "colab_type": "text"
      },
      "source": [
        "### Dataset iterator\n",
        "\n",
        "Dataset iterator is used to generate batches from parsed dataset (DatasetReader). Let's extract only `x` and `y` from parsed dataset and use them to predict sentence `y` by sentence `x`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWAeHlAOA_DG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from deeppavlov.core.data.data_learning_iterator import DataLearningIterator\n",
        "\n",
        "@register('personachat_iterator')\n",
        "class PersonaChatIterator(DataLearningIterator):\n",
        "    def split(self, *args, **kwargs):\n",
        "        for dt in ['train', 'valid', 'test']:\n",
        "            setattr(self, dt, self._to_tuple(getattr(self, dt)))\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_tuple(data):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            list of (x, y)\n",
        "        \"\"\"\n",
        "        return list(map(lambda x: (x['x'], x['y']), data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq01wsM8A_DI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "5b2748e2-294d-40c4-af9a-e158a2e6c9f9"
      },
      "source": [
        "iterator = PersonaChatIterator(data)\n",
        "batch_generator = iterator.gen_batches(5, 'train')\n",
        "batch = next(batch_generator)\n",
        "for x, y in zip(*batch):\n",
        "    print('x:', x)\n",
        "    print('y:', y)\n",
        "    print('----------')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: how are you doing today ?\n",
            "y: i am well , i am watching some anime and eating chocolate . how are you ?\n",
            "----------\n",
            "x: i have a bulldog what kind do you have\n",
            "y: i have felix the cat and emmy the terrier my pride and joys at home\n",
            "----------\n",
            "x: aw , that sounds awful . let me guess , not compatible ? or did this person cheat ? !\n",
            "y: he just does not like my glasses\n",
            "----------\n",
            "x: no . i wish i had a cat .\n",
            "y: i take my dog for walks a lot .\n",
            "----------\n",
            "x: mine are fine but they still expect me to eat dinner every sunday with them .\n",
            "y: boring . i am sorry for you , are you cook ?\n",
            "----------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlvA-mILA_DM",
        "colab_type": "text"
      },
      "source": [
        "### Tokenizer\n",
        "Splits utterance into tokens (words)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNuflNSOA_DN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "e90f54cf-b5e1-4521-f7f9-5b3f66496680"
      },
      "source": [
        "from deeppavlov.models.tokenizers.lazy_tokenizer import LazyTokenizer\n",
        "\n",
        "tokenizer = LazyTokenizer()\n",
        "tokenizer([\"I'd like to tokenize some text\"])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data] Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['I', \"'d\", 'like', 'to', 'tokenize', 'some', 'text']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kZr8KgUA_DR",
        "colab_type": "text"
      },
      "source": [
        "### Vocabulary\n",
        "\n",
        "Vocabulary object prepares mapping from tokens to token indices.\n",
        "It uses train data to build this mapping.\n",
        "\n",
        "We will implement DialogVocab (inherited from SimpleVocabulary) wich adds all tokens from `x` and `y` utterances to vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aqqc-aqA_DS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from deeppavlov.core.data.simple_vocab import SimpleVocabulary\n",
        "\n",
        "@register('dialog_vocab')\n",
        "class DialogVocab(SimpleVocabulary):\n",
        "    def fit(self, *args):\n",
        "        tokens = chain(*args)\n",
        "        super().fit(tokens)\n",
        "\n",
        "    def __call__(self, batch, **kwargs):\n",
        "        indices_batch = []\n",
        "        for utt in batch:\n",
        "            tokens = [self[token] for token in utt]\n",
        "            indices_batch.append(tokens)\n",
        "        return indices_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oSFMZ4TA_DV",
        "colab_type": "text"
      },
      "source": [
        "Let's create instance of DialogVocab. We define save and load paths, minimal frequence of tokens which are added to vocabulary and set of special tokens.\n",
        "\n",
        "Special tokens are:\n",
        "* <PAD\\> - padding\n",
        "* <SOS\\> - start of sequence\n",
        "* <EOS\\> - end of sequence\n",
        "* <UNK\\> - unknown token - token which is not presented in vocabulary\n",
        "\n",
        "And fit it on tokens from *x* and *y*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOh1iK__A_DW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "be1c9861-0cbc-4f39-d607-6eb1b1bc8ef5"
      },
      "source": [
        "vocab = DialogVocab(\n",
        "    save_path='./vocab.dict',\n",
        "    load_path='./vocab.dict',\n",
        "    min_freq=2,\n",
        "    special_tokens=('<PAD>','<SOS>', '<EOS>', '<UNK>',),\n",
        "    unk_token='<UNK>'\n",
        ")\n",
        "\n",
        "vocab.fit(tokenizer(iterator.get_instances(data_type='train')[0]), tokenizer(iterator.get_instances(data_type='train')[1]))\n",
        "vocab.save()\n",
        "\n",
        "PAD_idx = vocab._t2i['<PAD>']\n",
        "SOS_idx = vocab._t2i['<SOS>']\n",
        "assert PAD_idx == 0, 'this is required by tf.keras'"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-07-14 03:04:16.500 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 89: [saving vocabulary to /content/vocab.dict]\n",
            "I0714 03:04:16.500470 140328521566080 simple_vocab.py:89] [saving vocabulary to /content/vocab.dict]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79GwWlElA_Da",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82193667-5e01-4c73-f41d-4115e5bc810c"
      },
      "source": [
        "# number of words in vocabhttps://github.com/text-machine-lab/ciss2_materials/blob/master/tutorials/deeppavlov_track/Tutorial_Day_2_seq2seq.ipynb\n",
        "len(vocab)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11595"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4SCAqL7A_Di",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "d7b27f0c-1905-4da9-f572-820649cf1442"
      },
      "source": [
        "vocab.freqs.most_common(10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 103487),\n",
              " ('.', 101599),\n",
              " ('you', 48296),\n",
              " ('?', 43771),\n",
              " (',', 39500),\n",
              " ('a', 34214),\n",
              " ('to', 32105),\n",
              " ('do', 30574),\n",
              " ('is', 28579),\n",
              " ('my', 26953)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCcGBXX3A_Du",
        "colab_type": "text"
      },
      "source": [
        "One can use vocabulary to encode tokenized text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKxBIA-0A_Dv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9b5011f-8838-417d-9e3d-78422b245520"
      },
      "source": [
        "vocab([['<SOS>', 'this', 'is', 'tokenized', 'there_is_no_such_word_in_dataset', 'and_this', 'sentence', '<EOS>', '<PAD>', '<PAD>']])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 110, 12, 3, 3, 3, 6060, 2, 0, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mLNZ9gtA_D1",
        "colab_type": "text"
      },
      "source": [
        "## Padding\n",
        "\n",
        "To feed sequences of token indexes to neural model we should make their lengths equal. If sequence is too short we add <PAD\\> symbols to the end of sequence. If sequence is too long we just cut it.\n",
        "\n",
        "SentencePadder implements such behavior, it also adds <SOS\\> and <EOS\\> tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYBIVpy8A_D3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from deeppavlov.core.models.component import Component\n",
        "\n",
        "@register('sentence_padder')\n",
        "class SentencePadder(Component):\n",
        "    def __init__(self, length_limit, pad_token_id=0, start_token_id=1, end_token_id=2, *args, **kwargs):\n",
        "        self.length_limit = length_limit\n",
        "        self.pad_token_id = pad_token_id\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        for i in range(len(batch)):\n",
        "            batch[i] = batch[i][:self.length_limit]\n",
        "            batch[i] = [self.start_token_id] + batch[i] + [self.end_token_id]\n",
        "            batch[i] += [self.pad_token_id] * (self.length_limit + 2 - len(batch[i]))\n",
        "        return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnjKy6hZA_D8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "615434bb-e6af-414b-9b85-eb2cd31d00db"
      },
      "source": [
        "padder = SentencePadder(length_limit=6)\n",
        "padded = padder(vocab(tokenizer(['this is very very long sentence that does not fit',\n",
        "                                 'this one is short'])))\n",
        "padded"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 110, 12, 75, 75, 149, 6060, 2], [1, 110, 76, 12, 456, 2, 0, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saTMULPlA_EC",
        "colab_type": "text"
      },
      "source": [
        "To reverse mapping, just apply vocab again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__PW5gDPA_ED",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "eca33784-3a6a-4777-dbec-53d00f38b7a8"
      },
      "source": [
        "vocab(padded)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<SOS>', 'this', 'is', 'very', 'very', 'long', 'sentence', '<EOS>'],\n",
              " ['<SOS>', 'this', 'one', 'is', 'short', '<EOS>', '<PAD>', '<PAD>']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr3chbt8A_EI",
        "colab_type": "text"
      },
      "source": [
        "# Seq2seq model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvm3QuuBA_EJ",
        "colab_type": "text"
      },
      "source": [
        "![]()<img src=\"https://raw.githubusercontent.com/text-machine-lab/ciss2_materials/master/tutorials/deeppavlov_track/img/seq2seq_training.png\" width=\"700\"/>\n",
        "\n",
        "(image credit: Stanford cs224n)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWn4vYP9A_EJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_seq2seq_graph(input_ph, target_ph, build_encoder_fn, build_decoder_fn, hidden_size, vocab_size, emb_size, dropout_rate, is_training_ph):\n",
        "    \"\"\"\n",
        "    Args\n",
        "        x_ph: input tokens placeholder\n",
        "        y_ph: expected output tokens placeholder (used at training time for input feeding)\n",
        "        build_encoder: function to build encoder graph\n",
        "        build_decoder: function to build decoder graph\n",
        "        hidden_dim: size of encoder rnn\n",
        "        vocab_size: number of words in the vocabulary\n",
        "        emb_dim: embedding size\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor [batch_size, maxlen, decoder_output_dim]\n",
        "    \"\"\"\n",
        "    # embedding is shared between encoder and decoder\n",
        "    embedding_layer = tf.keras.layers.Embedding(vocab_size, emb_size)\n",
        "    dropout_layer = tf.keras.layers.Dropout(rate=dropout_rate)\n",
        "    mask = tf.cast(input_ph, tf.bool)\n",
        "\n",
        "    encoder_outputs_op, encoder_state_op = build_encoder_fn(\n",
        "        input_ph,\n",
        "        embedding_layer,\n",
        "        dropout_layer,\n",
        "        hidden_size,\n",
        "        is_training_ph\n",
        "    )\n",
        "    decoder_op = build_decoder_fn(\n",
        "        encoder_outputs_op,\n",
        "        encoder_state_op,\n",
        "        target_ph,\n",
        "        embedding_layer,\n",
        "        dropout_layer,\n",
        "        is_training_ph,\n",
        "        mask\n",
        "    )\n",
        "    logits_layer = tf.keras.layers.Dense(vocab_size)\n",
        "    logits_op = logits_layer(decoder_op)\n",
        "    return logits_op\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjADBsCnA_EL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_encoder(x_ph, embedding_layer, dropout_layer, hidden_dim, is_training_ph):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x_ph: input tokens placeholder\n",
        "        embedding_layer: tf.keras.Embedding object\n",
        "        dropout_layer: tf.keras.Dropout object\n",
        "        hidden_dim: size of rnn (also output size)\n",
        "        is_training_ph: is training mode flag\n",
        "\n",
        "    Returns:\n",
        "        encoder_outputs_op: tf.Tensor, [batch_size, maxlen, encoder_hidden_size]\n",
        "        encoder_state_op: tf.Tensor, [batch_size, encoder_hidden_size]\n",
        "    \"\"\"\n",
        "    # embed x, apply dropout if is_training\n",
        "    x_op = embedding_layer(x_ph)\n",
        "    x_op = dropout_layer(x_op, training=is_training_ph)\n",
        "\n",
        "    # make rnn layer and apply it to x\n",
        "    # rnn layer should return both encoded sequences and state\n",
        "    rnn_layer = tf.keras.layers.GRU(hidden_dim, return_sequences=True, return_state=True)\n",
        "    encoder_outputs_op, encoder_state_op = rnn_layer(x_op)\n",
        "\n",
        "    return encoder_outputs_op, encoder_state_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_ne6n0nA_EN",
        "colab_type": "text"
      },
      "source": [
        "Test encoder.\n",
        "\n",
        "shapes should be `[batch_size, maxlen, encoder_hidden_size]` and `[batch_size, encoder_hidden_size]`\n",
        "\n",
        "i.e `[3, 11, 17]` and `[3, 17]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DLq69kvA_EN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "a8e8a0fb-2c4c-4a9d-c886-1654ee88b02f"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "toy_batch_size = 3\n",
        "toy_vocab_size = 13\n",
        "toy_hidden_dim = 7\n",
        "toy_emb_size = 5\n",
        "toy_maxlen = 11\n",
        "\n",
        "toy_emb = tf.keras.layers.Embedding(toy_vocab_size, toy_emb_size, input_length=toy_maxlen)\n",
        "toy_dropout = tf.keras.layers.Dropout(rate=0.5)\n",
        "\n",
        "# toy_input = tf.cast(tf.random_uniform(shape=[toy_batch_size, toy_maxlen]) * toy_vocab_size, tf.int32)\n",
        "toy_input = tf.placeholder(tf.int64, [None, toy_maxlen])\n",
        "\n",
        "encoder_outputs_op, encoder_state_op = build_encoder(\n",
        "    toy_input, toy_emb, toy_dropout, toy_hidden_dim, True)\n",
        "\n",
        "if (encoder_outputs_op.shape[1:] == (toy_maxlen, toy_hidden_dim)):\n",
        "    print('Test 1 passed')\n",
        "else:\n",
        "    print('Problem with the shape')\n",
        "    print(f'Shape should be {(None, toy_maxlen, toy_hidden_dim)}')\n",
        "    print(f'But got {encoder_outputs_op.shape} instead')\n",
        "\n",
        "if (encoder_state_op.shape[1:] == (toy_hidden_dim)):\n",
        "    print('Test 2 passed')\n",
        "else:\n",
        "    print('Problem with the shape')\n",
        "    print(f'Shape should be {(None, toy_hidden_dim)}')\n",
        "    print(f'But got {encoder_state_op.shape} instead')\n",
        "\n",
        "encoder_outputs_op, encoder_state_op"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0714 03:04:16.681995 140328521566080 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0714 03:04:16.716256 140328521566080 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test 1 passed\n",
            "Test 2 passed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor 'gru/transpose_1:0' shape=(?, 11, 7) dtype=float32>,\n",
              " <tf.Tensor 'gru/while/Exit_3:0' shape=(?, 7) dtype=float32>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCzDGF-5A_EQ",
        "colab_type": "text"
      },
      "source": [
        "## Vanilla seq2seq decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1S8dyJ2A_EQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_decoder(\n",
        "        encoder_otputs_op,\n",
        "        encoder_state_op,\n",
        "        target_ph,\n",
        "        embedding_layer,\n",
        "        dropout_layer,\n",
        "        is_training_ph,\n",
        "        encoder_mask_ph=None):\n",
        "    \"\"\"Decoder without attention\n",
        "    it ignores encoder_otputs_op and uses only encoder_state_op to generate sequence\n",
        "\n",
        "    Args:\n",
        "        encoder_otputs_op: used only to get max_len\n",
        "        encoder_state_op: state of the encoder RNN, [batch_size, hidden_dim]\n",
        "        enc_mask_ph: ignored\n",
        "        target_ph: target placeholder, used at training time for input feeding\n",
        "        embedding_layer: tf.keras.Embedding object\n",
        "        dropout_layer: tf.keras.Dropout object\n",
        "        is_training_ph: is training mode flag\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor [batch_size, max_len, hidden_dim]\n",
        "    \"\"\"\n",
        "    _, max_len, hidden_dim = encoder_otputs_op.get_shape()\n",
        "    _, max_len, hidden_dim = _, max_len.value, hidden_dim.value\n",
        "\n",
        "    batch_size_op = tf.shape(encoder_otputs_op)[0]\n",
        "    vocab_size = embedding_layer.input_dim\n",
        "\n",
        "    # make decoder cell layer\n",
        "    decoder_cell = tf.keras.layers.GRUCell(hidden_dim)\n",
        "    # make decoder output projection layer (projects to vocabulary space)\n",
        "    decoder_output_proj_layer = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # first decoder input is start-of-sentence token\n",
        "    decoder_input_op = tf.ones([batch_size_op]) * SOS_idx\n",
        "    # first decoder state is last encoder state\n",
        "    decoder_state_op = encoder_state_op\n",
        "    # in this list we will store the logits of predicted sequence\n",
        "    output_logits = []\n",
        "\n",
        "    for i in range(max_len):\n",
        "        decoder_input_emb_op = embedding_layer(decoder_input_op)\n",
        "\n",
        "        # for some complicated reasons, we must to expand_dims on state\n",
        "        # decoder_cell returns output and states, in the case of GRU, states and output are the same\n",
        "        decoder_state_op, _ = decoder_cell(decoder_input_emb_op, tf.expand_dims(decoder_state_op, 1))\n",
        "\n",
        "        decoder_output_logit_op = decoder_output_proj_layer(decoder_state_op)\n",
        "        output_logits.append(decoder_output_logit_op)\n",
        "\n",
        "        # if training, use input feeding i.e. teacher forcing\n",
        "        decoder_input_op = tf.cond(is_training_ph,\n",
        "                                   lambda: target_ph[:, i],\n",
        "                                   lambda: tf.argmax(decoder_output_logit_op, axis=1))\n",
        "\n",
        "    output_logits_op = tf.stack(output_logits, axis=1)\n",
        "    return output_logits_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDiPxEvNA_ET",
        "colab_type": "text"
      },
      "source": [
        "Test decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKk0SKb6A_EU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "91600d94-b35e-4769-fdd6-559678698b04"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "toy_batch_size = 3\n",
        "toy_vocab_size = 13\n",
        "toy_hidden_dim = 7\n",
        "toy_emb_size = 5\n",
        "toy_maxlen = 11\n",
        "\n",
        "toy_emb = tf.keras.layers.Embedding(toy_vocab_size, toy_emb_size, input_length=toy_maxlen)\n",
        "toy_dropout = tf.keras.layers.Dropout(rate=0.5)\n",
        "\n",
        "toy_input = tf.placeholder(tf.int64, [None, toy_maxlen])\n",
        "toy_target = tf.placeholder(tf.int64, [None, toy_maxlen])\n",
        "toy_mask = tf.placeholder(tf.bool, [None, toy_maxlen])\n",
        "\n",
        "is_training_ph = tf.placeholder_with_default(tf.constant(True), shape=())\n",
        "\n",
        "encoder_outputs_op, encoder_state_op = build_encoder(\n",
        "    toy_input, toy_emb, toy_dropout, toy_hidden_dim, is_training_ph)\n",
        "\n",
        "toy_logits_op = build_decoder(\n",
        "    encoder_outputs_op, encoder_state_op, toy_target, toy_emb, toy_dropout, is_training_ph, toy_mask\n",
        ")\n",
        "\n",
        "if (toy_logits_op.shape[1:] == (toy_maxlen, toy_vocab_size)):\n",
        "    print('Test passed')\n",
        "else:\n",
        "    print('Problem with the shape')\n",
        "    print(f'Shape should be {(None, toy_maxlen, toy_vocab_size)}')\n",
        "    print(f'But got {toy_logits_op.shape} instead')\n",
        "\n",
        "toy_logits_op"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'stack:0' shape=(?, 11, 13) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrZq0zebA_EW",
        "colab_type": "text"
      },
      "source": [
        "Test full model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiVu8ECrA_EX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toy_logits_op = build_seq2seq_graph(\n",
        "    toy_input, toy_target, build_encoder, build_decoder, toy_hidden_dim, toy_vocab_size, toy_emb_size, 0.5, is_training_ph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc5qbM4yA_EY",
        "colab_type": "text"
      },
      "source": [
        "### Decoder with attention math\n",
        "\n",
        "Decoder is much more tricky then encoder, especially with attention.\n",
        "So it would be better for us to write down all decoder operations mathematically.\n",
        "\n",
        "Let $m$ be the length of a source sequence, $h$ be dimension of encoder output, $\\operatorname E \\in \\mathbb{R}^{ vocab\\_size \\times emb\\_size}$ - embedding matrix.\n",
        "\n",
        "Before encoding we have:\n",
        "$$\n",
        "\\mathbf{h}_i^{enc} \\in \\mathbb{R}^h - \\text{encoder output at i-th timestamp}\\\\\n",
        "\\mathbf{h}_m^{enc} - \\text{last encoder output (encoder state)}\\\\\n",
        "$$\n",
        "\n",
        "#### Zeroth step\n",
        "\n",
        "At the zeroth decoding step we sould construct decoder **input** and decoder **initial state**.\n",
        "Decoder **initial state** is encoder state.\n",
        "Decoder **input** is attention vector with SOS-token embedding as **query**.\n",
        "\n",
        "Let $sos$ be SOS-token index in embedding matrix.\n",
        "\n",
        "$$\n",
        "\\mathbf{h}_o^{dec} = \\mathbf{h}_m\\\\\n",
        "\\mathbf{e}_0 = \\operatorname{E}[sos]\\\\\n",
        "% \\mathbf{o}_0 = \\mathbf 0, \\mathbf{o}_0 \\in \\mathbb{R}^h\\\\\n",
        "% \\mathbf{h}_1^{dec} = \\operatorname{Decoder}([e_0; o_0])\n",
        "$$\n",
        "\n",
        "#### t-th step, t > 0\n",
        "\n",
        "At t-th step decoder **input** is attention vector with previous predicted token embedding as **query**.\n",
        "\n",
        "**Note:** at training time we use **teacher forcing** (it is also called input feeding) that means that instead of using previous predicted token decoder uses previous true token from target sequence.\n",
        "\n",
        "\n",
        "#### Attention\n",
        "\n",
        "When we got decoder state $h_1$, we can compute attention vector.\n",
        "\n",
        "Let $\\operatorname{W}_{attProj} \\in \\mathbb{R}^{h \\times h}$ be attention weighs, $\\mathbf{s}$ be attention scores, $\\mathbf{e}_t$ - embedding of the previous predicted token.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{s}_{t, i} &= (\\mathbf{e}_t)^T \\operatorname{W}_{attProj} \\mathbf{h}_i^{enc}\\\\\n",
        "\\mathbf{\\alpha}_t &= \\operatorname{Softmax}(\\mathbf{s}_t) \\text{  }\\\\\n",
        "\\mathbf{a}_t &= \\sum_i^m \\alpha_{t, i} \\mathbf{h}_i^{enc}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Or in terms of **query keys ans values**:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{s}_{t, i} &= (\\mathbf{q}_t)^T \\operatorname{W}_{attProj} \\mathbf{k}\\\\\n",
        "\\mathbf{\\alpha}_t &= \\operatorname{Softmax}(\\mathbf{s}_t) \\text{  }\\\\\n",
        "\\mathbf{a}_t &= \\sum_i^m \\alpha_{t, i} \\mathbf{v}\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVBK5R8xA_EZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_masked(values, mask):\n",
        "    masked_values = -np.inf * (1 - tf.cast(mask, tf.float32)) + values\n",
        "    return tf.nn.softmax(masked_values, 2)\n",
        "\n",
        "def build_decoder_with_attention(\n",
        "        encoder_outputs_op,\n",
        "        encoder_state_op,\n",
        "        target_ph,\n",
        "        embedding_layer,\n",
        "        dropout_layer,\n",
        "        is_training_ph,\n",
        "        encoder_mask_ph):\n",
        "    \"\"\"Decoder with Luong attention\n",
        "    https://arxiv.org/abs/1508.04025\n",
        "\n",
        "    Args:\n",
        "        encoder_otputs_op: used only to get max_len\n",
        "        encoder_state_op: state of the encoder RNN, [batch_size, hidden_dim]\n",
        "        enc_mask_ph: ignored\n",
        "        target_ph: target placeholder, used at training time for input feeding\n",
        "        embedding_layer: tf.keras.Embedding object\n",
        "        dropout_layer: tf.keras.Dropout object\n",
        "        is_training_ph: is training mode flag\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor [batch_size, max_len, hidden_dim]\n",
        "    \"\"\"    \n",
        "    _, max_len, hidden_dim = encoder_outputs_op.get_shape()\n",
        "    _, max_len, hidden_dim = _, max_len.value, hidden_dim.value\n",
        "\n",
        "    batch_size_op = tf.shape(encoder_outputs_op)[0]\n",
        "    vocab_size = embedding_layer.input_dim\n",
        "\n",
        "    # make decoder cell layer\n",
        "    decoder_cell = tf.keras.layers.GRUCell(hidden_dim)\n",
        "    # make decoder output projection layer (projects to vocabulary space)\n",
        "    decoder_output_proj_layer = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # first decoder input is start-of-sentence token\n",
        "    decoder_input_op = tf.ones([batch_size_op], dtype=tf.int64) * SOS_idx\n",
        "    # first decoder state is last encoder state\n",
        "    decoder_state_op = encoder_state_op\n",
        "    # in this list we will store the logits of predicted sequence\n",
        "    output_logits = []\n",
        "\n",
        "    # attention-related variables:\n",
        "    attention_proj_layer = tf.keras.layers.Dense(hidden_dim, use_bias=False)\n",
        "    attention_keys_op = attention_proj_layer(encoder_outputs_op)  # W_attProj @ h_enc\n",
        "    attention_values_op = encoder_outputs_op\n",
        "    attention_query_op = decoder_state_op\n",
        "\n",
        "    for i in range(max_len):\n",
        "        # compute input tensor for decoder rnn\n",
        "        decoder_input_emb_op = embedding_layer(decoder_input_op)\n",
        "\n",
        "        # apply attention with decoder_input_emb_op as query\n",
        "        attention_query_op = tf.expand_dims(decoder_state_op, 1)  # [batch_size, 1, hidden]\n",
        "        attention_scores_op = tf.matmul(attention_query_op, attention_keys_op, transpose_b=True)  # [batch_size, 1, maxlen]\n",
        "\n",
        "        attention_probs_op = softmax_masked(attention_scores_op, tf.expand_dims(encoder_mask_ph, 1))\n",
        "        attention_probs_op = tf.nn.softmax(attention_scores_op, 2)  # [batch_size, 1, maxlen]\n",
        "        attention_vec_op = tf.matmul(attention_probs_op, attention_values_op)  # [batch_size, 1, hidden]\n",
        "        attention_vec_op = tf.squeeze(attention_vec_op, 1)  # [batch_size, hidden]\n",
        "\n",
        "        # for some complicated reasons, we must to expand_dims(decoder_state_op, 1) on state\n",
        "        # decoder_cell returns output and states, in the case of GRU, states and output are the same\n",
        "        decoder_state_op, _ = decoder_cell(attention_vec_op, tf.expand_dims(decoder_state_op, 1))\n",
        "\n",
        "        decoder_output_logit_op = decoder_output_proj_layer(decoder_state_op)\n",
        "        output_logits.append(decoder_output_logit_op)\n",
        "\n",
        "        # if training, use input feeding i.e. teacher forcing\n",
        "        decoder_input_op = tf.cond(is_training_ph,\n",
        "                                   lambda: target_ph[:, i],\n",
        "                                   lambda: tf.argmax(decoder_output_logit_op, axis=1))\n",
        "\n",
        "    output_logits_op = tf.stack(output_logits, axis=1)\n",
        "    return output_logits_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCwybb0nA_Eb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "53093aee-257f-49c3-b35e-6e32d83722c8"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "toy_batch_size = 3\n",
        "toy_vocab_size = 13\n",
        "toy_hidden_dim = 7\n",
        "toy_emb_size = 5\n",
        "toy_maxlen = 11\n",
        "\n",
        "toy_emb = tf.keras.layers.Embedding(toy_vocab_size, toy_emb_size)\n",
        "toy_dropout = tf.keras.layers.Dropout(rate=0.5)\n",
        "\n",
        "toy_input = tf.placeholder(tf.int64, [None, toy_maxlen])\n",
        "toy_target = tf.placeholder(tf.int64, [None, toy_maxlen])\n",
        "toy_mask = tf.placeholder(tf.bool, [None, toy_maxlen])\n",
        "\n",
        "is_training_ph = tf.placeholder_with_default(tf.constant(True), shape=())\n",
        "\n",
        "encoder_outputs_op, encoder_state_op = build_encoder(\n",
        "    toy_input, toy_emb, toy_dropout, toy_hidden_dim, is_training_ph)\n",
        "\n",
        "toy_logits_op = build_decoder_with_attention(\n",
        "    encoder_outputs_op, encoder_state_op, toy_target, toy_emb, toy_dropout, is_training_ph, toy_mask\n",
        ")\n",
        "\n",
        "if (toy_logits_op.shape[1:] == (toy_maxlen, toy_vocab_size)):\n",
        "    print('Test passed')\n",
        "else:\n",
        "    print('Problem with the shape')\n",
        "    print(f'Shape should be {(toy_batch_size, toy_maxlen, toy_vocab_size)}')\n",
        "    print(f'But got {toy_logits_op.shape} instead')\n",
        "\n",
        "toy_logits_op"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'stack:0' shape=(?, 11, 13) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls6tHQuNA_Ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toy_logits_op = build_seq2seq_graph(\n",
        "    toy_input, toy_target, build_encoder, build_decoder_with_attention, toy_hidden_dim, toy_vocab_size, toy_emb_size, 0.5, is_training_ph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d6cIDlQA_Ef",
        "colab_type": "text"
      },
      "source": [
        "## Make model class and train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVTfcHG3A_Ef",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://bastings.github.io/annotated_encoder_decoder/images/bahdanau.png\" width=\"700\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V9jwKuXA_Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from deeppavlov.core.models.tf_model import TFModel\n",
        "# http://docs.deeppavlov.ai/en/master/_modules/deeppavlov/core/models/tf_model.html\n",
        "\n",
        "\n",
        "@register('seq2seq_57389')\n",
        "class Seq2Seq(TFModel):\n",
        "    def __init__(self, **kwargs):\n",
        "        # model hyperparameters\n",
        "        self.emb_size = kwargs['emb_size']\n",
        "        self.hidden = kwargs['hidden']\n",
        "        self.dropout = kwargs['dropout']\n",
        "        self.vocab_size = kwargs['vocab_size']\n",
        "        self.max_len = kwargs['max_len']\n",
        "\n",
        "        # optimization hyperparameters\n",
        "        self.grad_clip = kwargs.get('grad_clip', 5.)\n",
        "        self.learning_rate = kwargs.get('learning_rate', 1e-3)\n",
        "\n",
        "        # placeholders\n",
        "        self.input_ph = tf.placeholder(tf.int64, [None, self.max_len])\n",
        "        self.target_ph = tf.placeholder(tf.int64, [None, self.max_len])\n",
        "        self.is_training_ph = tf.placeholder_with_default(tf.constant(False), shape=())\n",
        "        self.target_mask_ph = tf.cast(self.target_ph > 0, tf.float32)\n",
        "\n",
        "        # graph\n",
        "        self.logits_op = build_seq2seq_graph(\n",
        "            self.input_ph,\n",
        "            self.target_ph,\n",
        "            build_encoder,\n",
        "            build_decoder,#_with_attention,\n",
        "            self.hidden,\n",
        "            self.vocab_size,\n",
        "            self.emb_size,\n",
        "            self.dropout,\n",
        "            self.is_training_ph)\n",
        "        self.predictions_op = tf.argmax(self.logits_op, axis=2)\n",
        "\n",
        "        self.loss = self._build_loss(self.input_ph, self.logits_op, self.target_mask_ph)\n",
        "\n",
        "        self.train_op = self.get_train_op(self.loss,\n",
        "                                          learning_rate=self.learning_rate,\n",
        "                                          optimizer=tf.train.AdamOptimizer,\n",
        "                                          clip_norm=self.grad_clip)\n",
        "\n",
        "        # create session and initialize graph variables\n",
        "        sess_config = tf.ConfigProto()\n",
        "        sess_config.gpu_options.allow_growth = True  # do not use all GPU memory at once\n",
        "        self.sess = tf.Session(config=sess_config)\n",
        "\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "        if self.save_path:\n",
        "            pass\n",
        "        if self.load_path is not None:\n",
        "            self.load()\n",
        "\n",
        "    def _build_loss(self, y_true, y_logits_pred, y_mask):\n",
        "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_logits_pred) * y_mask\n",
        "        loss = tf.reduce_sum(loss) / tf.reduce_sum(y_mask)\n",
        "        return loss\n",
        "\n",
        "    def _build_feed_dict(self, x, y=None):\n",
        "        feed_dict = {\n",
        "            self.input_ph: x,\n",
        "        }\n",
        "        if y is not None:\n",
        "            feed_dict.update({\n",
        "                self.target_ph: y,\n",
        "                self.is_training_ph: True,\n",
        "            })\n",
        "        return feed_dict\n",
        "\n",
        "    def train_on_batch(self, x, y):\n",
        "        feed_dict = self._build_feed_dict(x, y)\n",
        "        loss, _ = self.sess.run([self.loss, self.train_op], feed_dict=feed_dict)\n",
        "        return loss\n",
        "\n",
        "    def __call__(self, x):\n",
        "        feed_dict = self._build_feed_dict(x)\n",
        "        y_pred = self.sess.run(self.predictions_op, feed_dict=feed_dict)\n",
        "        return y_pred\n",
        "\n",
        "    def process_event(self, *args, **kwargs):\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsFdmPH_A_Ei",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "95689aad-9bb5-43c7-e446-599476589de4"
      },
      "source": [
        "model = Seq2Seq(emb_size=3, hidden=5, dropout=0.1, vocab_size=7, max_len=9, save_path=None)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-07-14 04:15:06.803 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 50: No load path is set for Seq2Seq!\n",
            "W0714 04:15:06.803834 140328521566080 serializable.py:50] No load path is set for Seq2Seq!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBE2njy1A_Ek",
        "colab_type": "text"
      },
      "source": [
        "### Postprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLIzGmNPA_El",
        "colab_type": "text"
      },
      "source": [
        "In postprocessing step we are going to remove all <PAD\\>, <SOS\\>, <EOS\\> tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOC4-PO1A_El",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@register('postprocessing')\n",
        "class SentencePostprocessor(Component):\n",
        "    def __init__(self, pad_token='<PAD>', start_token='<SOS>', end_token='<EOS>', *args, **kwargs):\n",
        "        self.pad_token = pad_token\n",
        "        self.start_token = start_token\n",
        "        self.end_token = end_token\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        for i in range(len(batch)):\n",
        "            batch[i] = ' '.join(self._postproc(batch[i]))\n",
        "        return batch\n",
        "    \n",
        "    def _postproc(self, utt):\n",
        "        if self.end_token in utt:\n",
        "            utt = utt[:utt.index(self.end_token)]\n",
        "        return utt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX08DXIrA_Ep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "postprocess = SentencePostprocessor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T7oDgK0A_Et",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padder(vocab([['hello', 'my', 'friend', 'there_is_no_such_word_in_dataset', 'and_this'], ['It']]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tDtR5KnA_Ex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padder = SentencePadder(length_limit=9 - 2)\n",
        "model = Seq2Seq(emb_size=3, hidden=5, dropout=0.1, vocab_size=7, max_len=9, save_path=None)\n",
        "postprocess(vocab(model(padder(vocab([['hello', 'my', 'friend', 'there_is_no_such_word_in_dataset', 'and_this']])))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pswOnPxA_E5",
        "colab_type": "text"
      },
      "source": [
        "### Create config file\n",
        "Let's put is all together in one config file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km6Dk6OGA_E5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "  \"dataset_reader\": {\n",
        "    \"class_name\": \"personachat_dataset_reader\",\n",
        "    \"data_path\": \"./personachat\"\n",
        "  },\n",
        "  \"dataset_iterator\": {\n",
        "    \"class_name\": \"personachat_iterator\",\n",
        "    \"shuffle\": True\n",
        "  },\n",
        "  \"chainer\": {\n",
        "    \"in\": [\"x\"],\n",
        "    \"in_y\": [\"y\"],\n",
        "    \"pipe\": [\n",
        "      {\n",
        "        \"class_name\": \"lazy_tokenizer\",\n",
        "        \"id\": \"tokenizer\",\n",
        "        \"in\": [\"x\"],\n",
        "        \"out\": [\"x_tokens\"]\n",
        "      },\n",
        "      {\n",
        "        \"class_name\": \"lazy_tokenizer\",\n",
        "        \"id\": \"tokenizer\",\n",
        "        \"in\": [\"y\"],\n",
        "        \"out\": [\"y_tokens\"]\n",
        "      },\n",
        "      {\n",
        "        \"class_name\": \"dialog_vocab\",\n",
        "        \"id\": \"vocab\",\n",
        "        \"save_path\": \"./vocab.dict\",\n",
        "        \"load_path\": \"./vocab.dict\",\n",
        "        \"min_freq\": 2,\n",
        "        \"special_tokens\": [\"<PAD>\",\"<SOS>\", \"<EOS>\", \"<UNK>\"],\n",
        "        \"unk_token\": \"<UNK>\",\n",
        "        \"fit_on\": [\"x_tokens\", \"y_tokens\"],\n",
        "        \"in\": [\"x_tokens\"],\n",
        "        \"out\": [\"x_tokens_ids\"]\n",
        "      },\n",
        "      {\n",
        "        \"ref\": \"vocab\",\n",
        "        \"in\": [\"y_tokens\"],\n",
        "        \"out\": [\"y_tokens_ids\"]\n",
        "      },\n",
        "      {\n",
        "        \"class_name\": \"sentence_padder\",\n",
        "        \"id\": \"padder\",\n",
        "        \"length_limit\": MAXLEN,\n",
        "        \"in\": [\"x_tokens_ids\"],\n",
        "        \"out\": [\"x_tokens_ids\"]\n",
        "      },\n",
        "      {\n",
        "        \"ref\": \"padder\",\n",
        "        \"in\": [\"y_tokens_ids\"],\n",
        "        \"out\": [\"y_tokens_ids\"]\n",
        "      },\n",
        "      {\n",
        "        \"class_name\": \"seq2seq_57389\",\n",
        "        \"id\": \"seq2seq_model\",\n",
        "        \"max_len\": \"#padder.length_limit+2\",\n",
        "        \"hidden\": 250,\n",
        "        \"emb_size\": 100,\n",
        "        \"vocab_size\": len(vocab),\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": 1e-3,\n",
        "        \"save_path\": \"./seq2seq_model_57389\",\n",
        "        \"load_path\": \"./seq2seq_model_57389\",\n",
        "        \"in\": [\"x_tokens_ids\"],\n",
        "        \"in_y\": [\"y_tokens_ids\"],\n",
        "        \"out\": [\"y_predicted_tokens_ids\"],\n",
        "      },\n",
        "      {\n",
        "        \"ref\": \"vocab\",\n",
        "        \"in\": [\"y_predicted_tokens_ids\"],\n",
        "        \"out\": [\"y_predicted_tokens\"]\n",
        "      },\n",
        "      {\n",
        "        \"class_name\": \"postprocessing\",\n",
        "        \"in\": [\"y_predicted_tokens\"],\n",
        "        \"out\": [\"y_predicted_tokens\"]\n",
        "      }\n",
        "    ],\n",
        "    \"out\": [\"y_predicted_tokens\"]\n",
        "  },\n",
        "  \"train\": {\n",
        "    \"log_every_n_batches\": 100,\n",
        "    \"val_every_n_epochs\": 500,\n",
        "    \"batch_size\": 64,\n",
        "    \"validation_patience\": 5,\n",
        "    \"epochs\": 10,\n",
        "    \"max_batches\": 1000,\n",
        "    \"metrics\": [\"bleu\"],\n",
        "  }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbXRtWc2A_E_",
        "colab_type": "text"
      },
      "source": [
        "### Interact with model using config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm66L8bkA_FA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGVM6KwtA_FF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1f1fdcc9-ad7b-41ad-dec9-22f475b8957e"
      },
      "source": [
        "model(['hi, how are you?', 'any ideas my dear friend?'])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['painters genre tina tina tina tina learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn',\n",
              " 'painters tina tina tina tina tina learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn learn']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQdUKP3fA_FL",
        "colab_type": "text"
      },
      "source": [
        "### Train model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oocWOWNhA_FM",
        "colab_type": "text"
      },
      "source": [
        "Run experiments with and without attention, with teacher forcing and without."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEtkMrlLA_FQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "0775b52f-3672-4f27-9f35-9b46cb4e3d55"
      },
      "source": [
        "from deeppavlov import train_model\n",
        "\n",
        "train_model(config)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-07-14 04:15:41.44 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 103: [loading vocabulary from /content/vocab.dict]\n",
            "I0714 04:15:41.044623 140328521566080 simple_vocab.py:103] [loading vocabulary from /content/vocab.dict]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-07-14 04:16:05.316 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 89: [saving vocabulary to /content/vocab.dict]\n",
            "I0714 04:16:05.316766 140328521566080 simple_vocab.py:89] [saving vocabulary to /content/vocab.dict]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIyZItGOA_FW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "114f2ef3-6a61-426f-c43e-4413e764fc55"
      },
      "source": [
        "model = build_model(config)\n",
        "\n",
        "model(['hi, how are you?', 'any ideas my dear friend?', 'okay, i agree with you', 'good bye!'])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-07-14 03:35:44.876 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 103: [loading vocabulary from /content/vocab.dict]\n",
            "I0714 03:35:44.876637 140328521566080 simple_vocab.py:103] [loading vocabulary from /content/vocab.dict]\n",
            "2019-07-14 03:36:05.529 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 52: [loading model from /content/seq2seq_model_57389]\n",
            "I0714 03:36:05.529874 140328521566080 tf_model.py:52] [loading model from /content/seq2seq_model_57389]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['frend shore shore approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching',\n",
              " 'corporation shore shore approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching',\n",
              " 'frend shore shore approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching',\n",
              " 'corporation shore shore approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imt4fzZ0A_FY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b158e951-f0f0-4701-ce75-10da6a7dec3e"
      },
      "source": [
        "model(['tell me about yourself'])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['corporation shore shore approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlGzoEE8dnCW",
        "colab_type": "text"
      },
      "source": [
        "## Telegram bot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YycXCdzVdl6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to interact with CLI\n",
        "from deeppavlov.core.commands.infer import interact_model\n",
        "# to interact with Telegram\n",
        "from deeppavlov.utils.telegram.telegram_ui import interact_model_by_telegram"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgmOCoafdv2e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "ad1a83b5-8eb7-4cf7-a568-c2911705e856"
      },
      "source": [
        "interact_model(config)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-07-14 03:31:51.928 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 103: [loading vocabulary from /content/vocab.dict]\n",
            "I0714 03:31:51.928510 140328521566080 simple_vocab.py:103] [loading vocabulary from /content/vocab.dict]\n",
            "2019-07-14 03:32:12.992 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 52: [loading model from /content/seq2seq_model_57389]\n",
            "I0714 03:32:12.992400 140328521566080 tf_model.py:52] [loading model from /content/seq2seq_model_57389]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x::hi\n",
            ">> corporation shore shore approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching approaching\n",
            "x::exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OVeDHIpdzRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interact_model_by_telegram(config,\n",
        "                           token='YOUR_TOKEN')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCYoDYzhA_Fc",
        "colab_type": "text"
      },
      "source": [
        "## Extra\n",
        "### Decoder with attention math 2 (more realistic case)\n",
        "\n",
        "This is typical NMT decoder with attention. It uses a lot of hacky tricks to make decoding a bit better.\n",
        "\n",
        "Decoder is much more tricky then encoder, especially with attention.\n",
        "So it would be better for us to write down all decoder operations mathematically.\n",
        "\n",
        "Let $m$ be the length of a source sequence, $h$ be dimension of encoder output, $\\operatorname E \\in \\mathbb{R}^{ vocab\\_size \\times emb\\_size}$ - embedding matrix.\n",
        "\n",
        "Before encoding we have:\n",
        "$$\n",
        "\\mathbf{h}_i^{enc} \\in \\mathbb{R}^h - \\text{encoder output at i-th timestamp}\\\\\n",
        "\\mathbf{h}_m^{enc} - \\text{last encoder output (encoder state)}\\\\\n",
        "$$\n",
        "\n",
        "#### Zeroth step\n",
        "\n",
        "At the zeroth decoding step we sould construct decoder **input** and decoder **initial state**.\n",
        "Decoder **initial state** is transformed (projected with matrix $\\mathbf{\\operatorname{W}}_h$) encoder state.\n",
        "Decoder **input** is _zero_ vector of size $h$.\n",
        "\n",
        "Let $sos$ be SOS-token index in embedding matrix.\n",
        "\n",
        "$$\n",
        "\\mathbf{h}_o^{dec} = \\operatorname{W}_h \\mathbf{h}_m, \\; \\operatorname{W}_h \\in \\mathbb{R}^{h \\times 2h} - \\text{decoder initial state is transformed encoder state}\\\\\n",
        "\\mathbf{e}_0 = \\operatorname{E}[sos]\\\\\n",
        "\\mathbf{o}_0 = \\mathbf 0, \\mathbf{o}_0 \\in \\mathbb{R}^h\\\\\n",
        "\\mathbf{h}_1^{dec} = \\operatorname{Decoder}([e_0; o_0])\n",
        "$$\n",
        "\n",
        "#### t-th step, t > 0\n",
        "\n",
        "At t-th step decoder **input** is concatenated combined-output vector $\\mathbf{o}_t$ (it is explained down this page in Attention paragraph) and previous predicted token embedding.\n",
        "\n",
        "**Note:** at training time we use **teacher forcing** (it is also called input feeding) that means that instead of using previous predicted token decoder uses previous true token from target sequence.\n",
        "\n",
        "\n",
        "#### Attention\n",
        "\n",
        "When we got decoder state $h_1$, we can compute attention vector.\n",
        "\n",
        "Let $\\operatorname{W}_{attProj} \\in \\mathbb{R}^{h \\times 2h}$ be attention weighs, $\\mathbf{s}$ be attention scores.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{s}_{t, i} &= (h_t^{dec})^T \\operatorname{W}_{attProj} h_i^{enc}\\\\\n",
        "\\mathbf{\\alpha}_t &= \\operatorname{Softmax}(\\mathbf{s}_t) \\text{  }\\\\\n",
        "\\mathbf{a}_t &= \\sum_i^m \\alpha_{t, i} \\mathbf{h}_i^{enc}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Then, decoder output is concatenated with attention vector and passed through a linear layer, tanh and dropout to attain combined-output vector $\\mathbf{o}_t$.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{u}_t = [a_t; h_t^{dec}]\n",
        "    \\text{   } \\; &where \\; \\text{  }\n",
        "        \\mathbf{u}_t \\in \\mathbb{R}^{3h \\times 1}\\\\\n",
        "\\mathbf{o}_t = \\operatorname{Dropout(tanh(W_u} \\mathbf{u}_t))\n",
        "    \\text{   } \\;  &where  \\text{   } \\; \n",
        "        \\operatorname{W}_u \\in \\mathbb{R}^{h \\times 3h}, \\mathbf{o}_t \\in \\mathbb{R}^{h \\times 1}\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEJ_POJQA_Fd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}